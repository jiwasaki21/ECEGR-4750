# -*- coding: utf-8 -*-
"""Logistic Regression Jacob.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEocQKAoiS8vRMIFmNwysxu74TXY7WMe

Jacob Iwasaki

ECEGR 4750

# Logistic-Regression Practice

We will use a version of the famous Titanic data set that requires very little cleaning.
"""

import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

"""Read in the data set."""

from google.colab import files

files.upload()

t_df = pd.read_csv('titanic_data.csv', index_col='PassengerId')
t_df = t_df.dropna()

"""Remove columns that don't make reasonable numeric predictors."""

t_df.drop(columns=['Name', 'Cabin', 'Ticket'], inplace=True)

"""Convert the remaining columns to use numeric labels."""

t_df['Sex'].replace(['male', 'female'], [1, 0], inplace=True)
t_df['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)

"""Extract the dependent and independent variables."""

X = t_df.drop(columns=['Survived'])
y = t_df['Survived']

"""Split training and test sets.

Notice that we are  _practicing to learn_, not creating a product, so we have not paid attention to validation vs. test.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# Checking the sizes of the training and test sets
print(f"Training Set Size (X_train): {len(X_train)} samples")
print(f"Test Set Size (X_test): {len(X_test)} samples")
print(f"Training Set Size (y_train): {len(y_train)} samples")
print(f"Test Set Size (y_test): {len(y_test)} samples")

# In terms of percentage
total_samples = len(X)
print(f"Training Set Percentage: {len(X_train) / total_samples * 100:.2f}%")
print(f"Test Set Percentage: {len(X_test) / total_samples * 100:.2f}%")

"""### Run everything up to this point and check the variable explorer for the following.
#### Do you have distinct training and test sets for the independent and dependent variables? Put the answer in your Jupyter notebook. Include the sizes of the sets in cardinality and percentage.

Independent variables are all the columns from the dataset except for the Survived column. The Dependent variable is the Survived column. The training set is 70% of the data. The test set is 30% of the data. The sizes and sets are in the code above.

#### Look at the two training sets and at least one test set to verify they contain what you expect.
Are there any issues? Put the answer in your Jupyter notebook. Include an explanation or discussion if necessary.

There are no issues. The two training sets and the test set contain the values that I expected.
"""

logmodel = sm.Logit(y_train, sm.add_constant(X_train)).fit(disp=False)
print(logmodel.summary())

"""### Are there any predictors that are not statistically significant in the conventional sense?
Put the answer in your Jupyter notebook.<p>
A variable is conventionally statistically significant if its _p_ value is less than 0.05. (Do you know why?)

The p-value measures the probability of observing your data, or something more extreme, if the null hypothesis is true. A p-value less than 0.05 indicates strong evidence against the null hypothesis, implying that the predictor is likely to have a real effect on the outcome. The predictors that are not statistically significant are the Pclass, Age, SibSp, Parch, Fare, Embarked.

### What variable is particularly strong in predicting survival?
Put the answer in your Jupyter notebook.

The variable that is particularly strong in predicting survival is Sex because the p-value is 0, which is highly statisitcally significant.

### What does a negative coefficient imply and why?
Put the answer in your Jupyter notebook.

A negative coefficient in logisitic regression indicates an inverse relationship between the predictor and the outcome. As the predictor's value increases, the probability of the outcome being 1 decreases. This means that higher values of the predictor are linked to lower odds of the event occurring.

### Based on your discussion, first think about what other variable ought to be a decent predictor?

### Next, check the report output to see if that was the case.
Enter what variable you thought might be a good predictor and whether that turned out to be the case.

The variable I think might be a good predictor is Age. Looking at the results, Age p-values is 0.067 which is greater the the threshold of 0.05. Age is not statistically significant in this model.

## Next, we wil learn about the quality of our predictions on the test set.
"""

from sklearn.metrics import accuracy_score, confusion_matrix

# Form our predictions, convert continuous [0, 1] predictions to binary
predictions = logmodel.predict(sm.add_constant(X_test))
bin_predictions = [1 if x >= 0.5 else 0 for x in predictions]

# We can now assess the accuracy and print out the confusion matrix
print(accuracy_score(y_test, bin_predictions))
print(confusion_matrix(y_test, bin_predictions))

"""## Discussion

### There is another way to evaluate our model... for a variety of thresholds.
"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, predictions)
roc_auc = roc_auc_score(y_test, predictions)

plt.plot(fpr, tpr, label='ROC Curve (area = %0.3f)' % roc_auc)
plt.title('ROC Curve (area = %0.3f)' % roc_auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')