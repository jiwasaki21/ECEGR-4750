# -*- coding: utf-8 -*-
"""ECEGR 4750 Lab 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14E_X2E-lSz5G__Xv2uMTK86EvrvO1hB9

Jacob Iwasaki
10/17/24
Lab 1
"""

from google.colab import files
files.upload()

"""# Regular vs. Multiple Linear Regression"""

# Step 1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

"""## New Data"""

# Step 2
df = pd.read_csv("Advertising.csv", index_col = 0)

"""## Regular Linear Regression with One of the Dataset's Predictors"""

# Step 3
# First, do linear regression one at a time using two predictors and find the R-squared value for each.
y = df.Sales
X = df.TV
X = sm.add_constant(X)

# Step 4
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)

# Step 5
plt.figure()
plt.scatter(df.TV, df.Sales)
plt.xlabel("TV")
plt.ylabel("Sales")

"""## Regular Linear Regression with Another Predictor"""

# Step 6
X = df.Radio
X = sm.add_constant(X)
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)
plt.figure()
plt.scatter(df.Radio, df.Sales)
plt.xlabel("Radio")
plt.ylabel("Sales")

"""## Interpreting the Plots and R-squared Values

# Step 7
* We can tell from the plots that 'TV' correlates better with 'Sales' than does 'Radio'.
* The R-squared value for 'TV' is 61.2%.
* The R-squared value for 'Radio' is 33.2%.
* Together, should they explain 99.4% of the variation in 'Sales'?

No because there might be overlap between TV and Radio's influence on Sales causing the variance to be less then the sum of the two R-squared values.

## Multiple linear regression is just linear regression with more than one independent variable.
"""

# Step 8
X = df[["TV", "Radio"]]
X = sm.add_constant(X)
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)

"""## Let's see the multi regression in a three-D plot."""

# Step 9
from mpl_toolkits.mplot3d import Axes3D

# Use the ranges of values in the 'TV' and 'Radio' variables to work out the first two axes for the 3D plot.
x_axis, y_axis = np.meshgrid(np.linspace(X.TV.min(), X.TV.max(), 100), np.linspace(X.Radio.min(), X.Radio.max(), 100))

z_axis = (lr_model.params[0]) + (lr_model.params[1]) * (x_axis) + (lr_model.params[2]) * (y_axis)

# Generate 3D axes.
fig = plt.figure(figsize = (12, 8))
ax = fig.add_subplot(projection='3d', azim=-100)

# finally the plotting
ax.plot_surface(x_axis, y_axis, z_axis, cmap = plt.cm.coolwarm, alpha = 0.5, linewidth = 0)
ax.scatter(X.TV, X.Radio, y)
ax.set_xlabel("TV")
ax.set_ylabel("Radio")
ax.set_zlabel("Sales")

"""You can see that the "linear discriminant" our regression found is a plane.

## Interpreting the R-squared Values

# Step 10
* We get an R-squared value of 89.7%.
* This is less than 94.4%.
* Why would that be the case? (Make sure to think about the plots, not just the R-squared numbers.)

The R-squared value of 89.7% is less than the 94.4% because of the overlapping and redundancy. If you look at both plots, some points on the TV graph may be overlapping with points on the Radio graph causing the R value to be lower.

# Part 2
"""

import pandas as pd

# Load data
from google.colab import files
uploaded = files.upload()  # Upload your CSV file
df = pd.read_csv('allgreens.csv')  # Change filename accordingly

df.head()  # Display first few rows

corr_advertising = df['X1'].corr(df['X4'])
print(f"Correlation between net sales and advertising: {corr_advertising}")

corr_competitors = df['X1'].corr(df['X6'])
print(f"Correlation between net sales and number of competitors: {corr_competitors}")

import statsmodels.api as sm

# Define predictors (X2, X3, X4, X5, X6) and target variable (X1 as Sales)
X = df[['X2', 'X3', 'X4', 'X5', 'X6']]  # Independent variables
y = df['X1']  # Dependent variable (annual net sales)

# Add a constant term for the intercept
X = sm.add_constant(X)

# Fit the linear regression model using OLS (Ordinary Least Squares)
lr_model = sm.OLS(y, X).fit()

# Print the model summary and parameters
print(lr_model.summary())
print(lr_model.params)

"""
The OLS model explains 99.3% of the variance in annual net sales, with advertising, square footage, and district size showing positive effects, while more competitors reduce sales. Regularization is not needed because the OLS model is already a good fit. The reason for this is because there is not much overlapping or severe overfitting which fluctuates the data."""

from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler

# Define predictors and target variable
X = df[['X2', 'X3', 'X4', 'X5', 'X6']]
y = df['X1']

# Scale the predictors to ensure consistent regularization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit Lasso Regression with alpha=0.1 (can be adjusted)
lasso = Lasso(alpha=0.1)
lasso.fit(X_scaled, y)

print("Lasso Coefficients:", lasso.coef_)

r_squared = lasso.score(X_scaled, y)
print(f"Lasso R-squared: {r_squared}")

"""Lasso and OLS produced the same RMSE (15.57), indicating no improvement in predictive performance with regularization. Although Lasso increased the magnitudes of the coefficients, none were reduced to zero, suggesting all predictors contribute meaningfully. This outcome implies that OLS was already a good fit, and regularization did not provide additional benefits, likely because the data does not suffer from severe overfitting or redundant predictors.

# Part 3

2.1.) The response variable for california_housing_train.csv would be median house value.

2.2a) Former case
The key predictors for median house value are median income, total rooms, and housing median age. Higher median income leads to increased housing prices, as it allows residents to afford more expensive homes. Total rooms matter because larger homes are more desirable and command higher prices. Housing median age affects value as newer homes tend to have modern features and higher prices, while older homes may need repairs, reducing their market value.

2.2b) Latter case
Data analysis can validate these predictors. Scatter plots would likely show positive correlations between median income and house value, and between total rooms and house value, while an inverse relationship may exist with housing median age. Correlation coefficients would quantify these relationships, indicating strong positive values for income and total rooms, and a negative value for age. Box plots could further illustrate how median house values vary with different income levels, room counts, and housing ages.
"""

# coding: utf-8

import numpy as np
from sklearn import datasets, linear_model
import matplotlib.pyplot as plt


# Load the diabetes data set.
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)

# Use only one feature.
diabetes_X = diabetes_X[:, np.newaxis, 2]

# Create the linear-regression object.
regr_linear = linear_model.LinearRegression()
regr_Lasso = linear_model.Lasso(alpha=0.1)
regr_Ridge = linear_model.Ridge(alpha=0.1)

# Train the model using the training set.
regr_linear.fit(diabetes_X, diabetes_y)
regr_Lasso.fit(diabetes_X, diabetes_y)
regr_Ridge.fit(diabetes_X, diabetes_y);

# Get the resulting coefficients.
print("linear coefficients: \n", regr_linear.coef_)
print("linear intercept: \n", regr_linear.intercept_)
print("LASSO coefficients: \n", regr_Lasso.coef_)
print("LASSO intercept: \n", regr_Lasso.intercept_)
print("ridge coefficients: \n", regr_Ridge.coef_)
print("ridge intercept: \n", regr_Ridge.intercept_)

# Get the coefficients and intercepts for plain old linear regression.
lin_coefficients = regr_linear.coef_
lin_intercept = regr_linear.intercept_

# Construct the equation string.
equation = f"y = {lin_intercept:.2f}"
for i, coef in enumerate(lin_coefficients):
    equation += f" + {coef:.2f} * x{i+1}"

print(equation)


# Get the coefficients and intercepts for LASSO-regularized linear regression.
las_coefficients = regr_Lasso.coef_
las_intercept = regr_Lasso.intercept_

# Construct the equation string.
equation = f"y = {las_intercept:.2f}"
for i, coef in enumerate(las_coefficients):
    equation += f" + {coef:.2f} * x{i+1}"

print(equation)


# Get the coefficients and intercepts for ridge-regularized linear regression.
rid_coefficients = regr_Ridge.coef_
rid_intercept = regr_Ridge.intercept_

# Construct the equation string.
equation = f"y = {rid_intercept:.2f}"
for i, coef in enumerate(rid_coefficients):
    equation += f" + {coef:.2f} * x{i+1}"

print(equation)

# Generate predictions with each option.
y_pred = regr_linear.predict(diabetes_X)
y_L_pred = regr_Lasso.predict(diabetes_X)
y_r_pred = regr_Ridge.predict(diabetes_X)

# Plot the data points and the regression line.
plt.figure(figsize=(10, 10))  # 10" by 10"
plt.scatter(diabetes_X, diabetes_y, color='blue', label='data')
plt.plot(diabetes_X, y_pred, color='red', label='ridge-regularized regression line')
plt.plot(diabetes_X, y_L_pred, color='green', label='LASSO-regularized regression line')
plt.plot(diabetes_X, y_r_pred, color='cyan', label='ridge-regularized regression line')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

"""Explain how useful you think this model is. Explain your answer both in terms of the plot and in terms of the choices we made about the model complexity for purposes of this exercise.

Hint:
In what way is the LASSO line more conservative than the unregularized-regression line? Similarly, how's the ridge line more conservative than the other two?

The models—linear regression, LASSO, and Ridge—each provide valuable insights into the dataset, both through their visual representations in the plot and their underlying complexity.

### Plot Insights:
1. **Linear Regression Line**: This line fits the data points closely but is sensitive to outliers, which may lead to overfitting and reduced generalizability.
2. **LASSO Regression Line**: The LASSO line appears more conservative, reflecting its focus on the most relevant features. By shrinking less important coefficients to zero, it simplifies the model, reducing the risk of overfitting.
3. **Ridge Regression Line**: The Ridge line is smoother, indicating that it retains all features while controlling their influence. This makes it more stable against noise and multicollinearity but does not eliminate any predictors.

### Model Complexity Insights:
1. **Linear Regression**: This model is straightforward and interpretable but lacks the flexibility to manage overfitting, particularly in complex datasets.
2. **LASSO Regression**: By introducing a penalty on coefficient sizes, LASSO promotes sparsity and emphasizes important features, resulting in a more conservative model that can generalize better to unseen data.
3. **Ridge Regression**: This model smooths the impact of all features by shrinking coefficients, making it useful in cases where multicollinearity exists. It balances complexity and stability by including all variables but dampening their effect.

In summary, the visual representation of each model in the plot highlights their strengths and weaknesses, while the choices made regarding model complexity influence their predictive capabilities. Linear regression offers simplicity, LASSO enhances feature selection, and Ridge ensures stability, allowing for nuanced interpretations of the dataset.
"""

from google.colab import files
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.linear_model import Ridge

# Upload the dataset
uploaded = files.upload()

# Load the dataset
df = pd.read_csv("california_housing_train.csv", index_col=0)

# Define the target variable
y = df['median_house_value']

# Identify the best and second-best predictors
correlations = df.corr()['median_house_value'].drop('median_house_value')
best_predictor = correlations.idxmax()  # Best predictor
second_best_predictor = correlations[correlations < correlations.max()].idxmax()  # Second best predictor

# Print the best and second-best predictors
print(f"Best Predictor: {best_predictor}")
print(f"Second Best Predictor: {second_best_predictor}")

# Define the predictors
X_best = df[[best_predictor]]
X_second_best = df[[second_best_predictor]]
X_both = df[[best_predictor, second_best_predictor]]
X_all = df[['median_income', 'total_rooms', 'housing_median_age']]  # Adjust based on your findings

# Function to fit OLS model and return the model
def fit_ols_model(X):
    X = sm.add_constant(X)  # Add constant for intercept
    model = sm.OLS(y, X).fit()
    return model

# Function to fit Ridge model and return the model
def fit_ridge_model(X):
    X = sm.add_constant(X)  # Add constant for intercept
    model = Ridge(alpha=1.0)  # Default alpha
    model.fit(X, y)
    return model, X  # Return the model and X with constant

# Function to print regression summary
def print_regression_summary(model_ols, model_ridge, X):
    print(model_ols.summary())
    print("\nRidge R-squared:", model_ridge.score(X, y))  # Print Ridge R-squared
    print("\n" + "="*80 + "\n")  # Separator for readability

# Fit models and display summaries for each case
print("### Best Predictor Regression ###")
model_best_ols = fit_ols_model(X_best)
model_best_ridge, X_best_with_const = fit_ridge_model(X_best)
print_regression_summary(model_best_ols, model_best_ridge, X_best_with_const)

print("### Second Best Predictor Regression ###")
model_second_best_ols = fit_ols_model(X_second_best)
model_second_best_ridge, X_second_best_with_const = fit_ridge_model(X_second_best)
print_regression_summary(model_second_best_ols, model_second_best_ridge, X_second_best_with_const)

print("### Both Best and Second Best Predictors Regression ###")
model_both_ols = fit_ols_model(X_both)
model_both_ridge, X_both_with_const = fit_ridge_model(X_both)
print_regression_summary(model_both_ols, model_both_ridge, X_both_with_const)

print("### All Predictors Regression ###")
model_all_ols = fit_ols_model(X_all)
model_all_ridge, X_all_with_const = fit_ridge_model(X_all)
print_regression_summary(model_all_ols, model_all_ridge, X_all_with_const)