# -*- coding: utf-8 -*-
"""multiple_lin_reg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13v87T_F3Ai-H1lFd6yxEWkyBm403bPhz
"""

from google.colab import files
files.upload()

"""# Regular vs. Multiple Linear Regression"""

# Step 1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

"""## New Data"""

# Step 2
df = pd.read_csv("Advertising.csv", index_col = 0)

"""## Regular Linear Regression with One of the Dataset's Predictors"""

# Step 3
# First, do linear regression one at a time using two predictors and find the R-squared value for each.
y = df.Sales
X = df.TV
X = sm.add_constant(X)

# Step 4
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)

# Step 5
plt.figure()
plt.scatter(df.TV, df.Sales)
plt.xlabel("TV")
plt.ylabel("Sales")

"""## Regular Linear Regression with Another Predictor"""

# Step 6
X = df.Radio
X = sm.add_constant(X)
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)
plt.figure()
plt.scatter(df.Radio, df.Sales)
plt.xlabel("Radio")
plt.ylabel("Sales")

"""## Interpreting the Plots and R-squared Values

# Step 7
* We can tell from the plots that 'TV' correlates better with 'Sales' than does 'Radio'.
* The R-squared value for 'TV' is 61.2%.
* The R-squared value for 'Radio' is 33.2%.
* Together, should they explain 99.4% of the variation in 'Sales'?

No because there might be overlap between TV and Radio's influence on Sales causing the variance to be less then the sum of the two R-squared values.

## Multiple linear regression is just linear regression with more than one independent variable.
"""

# Step 8
X = df[["TV", "Radio"]]
X = sm.add_constant(X)
lr_model = sm.OLS(y,X).fit()
print(lr_model.summary())
print(lr_model.params)

"""## Let's see the multi regression in a three-D plot."""

# Step 9
from mpl_toolkits.mplot3d import Axes3D

# Use the ranges of values in the 'TV' and 'Radio' variables to work out the first two axes for the 3D plot.
x_axis, y_axis = np.meshgrid(np.linspace(X.TV.min(), X.TV.max(), 100), np.linspace(X.Radio.min(), X.Radio.max(), 100))

z_axis = (lr_model.params[0]) + (lr_model.params[1]) * (x_axis) + (lr_model.params[2]) * (y_axis)

# Generate 3D axes.
fig = plt.figure(figsize = (12, 8))
ax = fig.add_subplot(projection='3d', azim=-100)

# finally the plotting
ax.plot_surface(x_axis, y_axis, z_axis, cmap = plt.cm.coolwarm, alpha = 0.5, linewidth = 0)
ax.scatter(X.TV, X.Radio, y)
ax.set_xlabel("TV")
ax.set_ylabel("Radio")
ax.set_zlabel("Sales")

"""You can see that the "linear discriminant" our regression found is a plane.

## Interpreting the R-squared Values

# Step 10
* We get an R-squared value of 89.7%.
* This is less than 94.4%.
* Why would that be the case? (Make sure to think about the plots, not just the R-squared numbers.)

The R-squared value of 89.7% is less than the 94.4% because of the overlapping and redundancy. If you look at both plots, some points on the TV graph may be overlapping with points on the Radio graph causing the R value to be lower.
"""

