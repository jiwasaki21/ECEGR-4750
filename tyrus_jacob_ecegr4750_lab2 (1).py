# -*- coding: utf-8 -*-
"""Tyrus_Jacob ECEGR4750 Lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bv7PGlVUVtf7TqZ6oL4TVV3l4RPJNP7P

ECEGR 4750 Lab2
November 7, 2024
Jacob Iwasaki, Tyrus Viela

2.
3.

There were times where the conditionals were 0, so to fix this, we put a small number for example 0.001 / 12.001. We initially did our calculations wrong becuase of counting mistakes but we were able to fix it.
Jacob: Some challenges were that he made some calculations mistakes when calculating the probabilities. He did the probability calculations and help edit the Bayes code.
Tyrus: Some challenges were counting mistakes when helping calculate the probability calculations. He edited the Bayes code and helped do the initial calculations.
"""

import numpy as np
import pandas as pd
from google.colab import files
files.upload()
carData = pd.read_csv('carDataNL.csv')
print(carData.shape)  # gives a tuple with the shape of data frame

print(carData.head())

splitRatio = .85

price_labels = ['low', 'medium', 'high', 'luxury']
maint_labels = ['low_cost', 'med_cost', 'high_cost', 'very_high_cost']
trunk_labels = ['small', 'med', 'big']
safety_labels = ['poor', 'ave', 'good']

# Do the shuffle!
reCarData = carData.reindex(np.random.permutation(carData.index))
shuffCarData = reCarData.reset_index()

shuffCarData

shuffCarData.pop('index')

shuffCarData

# function definition: getting the counts (frequencies of occurrence)
def count(data,colname,label,target):    # for whichever data set, pick a column and one value in that column
                                         # and an output label (target) to compare against
    condition = (data[colname] == label) & (data['RATING'] == target)
    print(data[condition])
    return len(data[condition])

# list of predicted values
predicted = []

# dictionary to store probabilities
probabilities = {0:{},1:{}}

#length of current training set
train_len = int(splitRatio * len(shuffCarData))
print(train_len)

#splitting the data into training and test sets
training = shuffCarData.iloc[:train_len,:] # the first train_len rows, all columns

# debugging step: looking at the training set
training

test_X = shuffCarData.iloc[train_len:,:-1] # features in the test set
test_y = shuffCarData.iloc[train_len:,-1]  # targets (the last entry in each row; i.e., the last column)

# debugging
print(test_X)

# debugging
print(test_y)

# checking for dimension match
print(test_X.shape)
print(test_y.shape)

# counts of prior 'LOW' and 'HIGH' labels (targets)
count_LOW = count(training, 'RATING', 'LOW', 'LOW')
count_HIGH = count(training, 'RATING', 'HIGH', 'HIGH')

prior_LOW = count_LOW / len(training)
prior_HIGH = count_HIGH / len(training)

print('The prior \'LOW\' count is', count_LOW)
print('and \'HIGH\',',count_HIGH, '.')
print('The prior probability of \'LOW\' is', prior_LOW)
print('and of \'HIGH\',', prior_HIGH, '.')

# TRAINING: learning the priors and class-conditionals from the training set
for col in training.columns[:-1]:
    probabilities[0][col] = {}
    probabilities[1][col] = {}

    if col == "price":
        for k in price_labels:
            count_k_LOW = count(training, col, k, 'LOW')
            count_k_HIGH = count(training, col, k, 'HIGH')
            probabilities[0][col][k] = count_k_LOW / count_LOW
            probabilities[1][col][k] = count_k_HIGH / count_HIGH
            probabilities[0][col][k] += 0.001 # throwing in a quick-and-dirty approximation of the
            probabilities[1][col][k] += 0.001 # "m estimate" for avoiding zeros in the product
    else:
        if col == "maint_cost":
            for k in maint_labels:
                count_k_LOW = count(training, col, k, 'LOW')
                count_k_HIGH = count(training, col, k, 'HIGH')
                probabilities[0][col][k] = count_k_LOW / count_LOW
                probabilities[1][col][k] = count_k_HIGH / count_HIGH
                probabilities[0][col][k] += 0.001 # throwing in a quick-and-dirty approximation of the
                probabilities[1][col][k] += 0.001 # "m estimate" for avoiding zeros in the product
        else:
            if col == "trunk":
                for k in trunk_labels:
                    count_k_LOW = count(training, col, k, 'LOW')
                    count_k_HIGH = count(training, col, k, 'HIGH')
                    probabilities[0][col][k] = count_k_LOW / count_LOW
                    probabilities[1][col][k] = count_k_HIGH / count_HIGH
                    probabilities[0][col][k] += 0.001 # throwing in a quick-and-dirty approximation of the
                    probabilities[1][col][k] += 0.001 # "m estimate" for avoiding zeros in the product
            else:
                for k in safety_labels:
                    count_k_LOW = count(training, col, k, 'LOW')
                    count_k_HIGH = count(training, col, k, 'HIGH')
                    probabilities[0][col][k] = count_k_LOW / count_LOW
                    probabilities[1][col][k] = count_k_HIGH / count_HIGH
                    probabilities[0][col][k] += 0.001 # throwing in a quick-and-dirty approximation of the
                    probabilities[1][col][k] += 0.001 # "m estimate" for avoiding zeros in the product

import pprint

# Sample dictionary for demonstration
pp = pprint.PrettyPrinter(indent=4)

# Format each value in the nested dictionaries and print
formatted_probabilities = {
    key: {inner_key: {sub_key: "{:.3f}".format(sub_value) for sub_key, sub_value in inner_value.items()}
           for inner_key, inner_value in value.items()}
    for key, value in probabilities.items()
}

pp.pprint(formatted_probabilities)

# TESTING: reading rows from the test set, checking the value of each feature and comparing with the target outcome
for row in range(len(test_X)):
    prod_LOW = prior_LOW
    prod_HIGH = prior_HIGH

    for feature in test_X.columns:
        # Check if the feature value exists in the probabilities dictionary:
        if test_X[feature].iloc[row] in probabilities[0][feature]:
            prod_LOW *= probabilities[0][feature][test_X[feature].iloc[row]]
        else:
            prod_LOW *= 0  # or some default value if the feature value is not found

        if test_X[feature].iloc[row] in probabilities[1][feature]:
            prod_HIGH *= probabilities[1][feature][test_X[feature].iloc[row]]
        else:
            prod_HIGH *= 0  # or some default value if the feature value is not found

    # predicting the outcome
    if prod_LOW > prod_HIGH:
        predicted.append('LOW')
    else:
        predicted.append('HIGH')

# Initialize correct and incorrect counts outside the loop
correct = 0
incorrect = 0

for j in range(len(test_y)):
    if predicted[j] == 'LOW':
        if test_y.iloc[j] == 'LOW':
            correct += 1
        else:
            incorrect += 1
    else:
        if test_y.iloc[j] == 'LOW':
            incorrect += 1  # This should be incorrect since predicted is 'HIGH'
        else:
            correct += 1  # This should be correct since predicted is 'HIGH'

print('The accuracy is', correct / (correct + incorrect))

from sklearn.metrics import confusion_matrix
import pandas as pd

# Using the 'LOW' and 'HIGH' labels for the matrix
class_labels = ["LOW", "HIGH"]

# Generate the confusion matrix
conf_matrix = confusion_matrix(test_y, predicted, labels=class_labels)

# Convert the confusion matrix to a DataFrame for labeled display
conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)

# Display the confusion matrix
print("Confusion Matrix:\n", conf_matrix_df)

"""The hand-made confusion matrix looks the same as the matrix in Python.

We both worked on Part 2 equally, even making the hand-made confusion matrix and classification of the five vectors with 0/1 loss.

## Part 3.
"""

import numpy as np
import pandas as pd
from google.colab import files
files.upload()
motor_vehicles_data = pd.read_csv('motor_vehicles.csv')

# Shuffle the dataset
motor_vehicles_data = motor_vehicles_data.sample(frac=1).reset_index(drop=True)

# Split the data: 75% training and 25% testing
split_ratio = 0.75
train_len = int(split_ratio * len(motor_vehicles_data))
training_data = motor_vehicles_data.iloc[:train_len, :]
test_data = motor_vehicles_data.iloc[train_len:, :]

# Separate features and labels
test_X = test_data.iloc[:, :-1]  # Test features, all columns except the last
test_y = test_data.iloc[:, -1]   # Test labels, the last column (assuming labels are in the last column)

# Define label categories if they align with your previous dataset
price_labels = ['low', 'medium', 'high', 'luxury']
maint_labels = ['low_cost', 'med_cost', 'high_cost', 'very_high_cost']
trunk_labels = ['small', 'med', 'big']
safety_labels = ['poor', 'ave', 'good']

# Initialize dictionaries for priors and probabilities
priors = {}
probabilities = {0: {}, 1: {}}  # Assuming 0 corresponds to 'LOW' and 1 to 'HIGH'

# Calculate priors for each class ('LOW' and 'HIGH')
for class_label in ['LOW', 'HIGH']:
    priors[class_label] = len(training_data[training_data['RATING'] == class_label]) / len(training_data)

# Calculate conditional probabilities for each feature and class
for class_label in ['LOW', 'HIGH']:
    probabilities[class_label] = {}
    class_data = training_data[training_data['RATING'] == class_label]

    for feature in training_data.columns[:-1]:  # Exclude the label column 'RATING'
        probabilities[class_label][feature] = {}

        # Use the predefined labels based on your previous features
        if feature == "price":
            labels = price_labels
        elif feature == "maint_cost":
            labels = maint_labels
        elif feature == "trunk":
            labels = trunk_labels
        elif feature == "safety":
            labels = safety_labels
        else:
            labels = training_data[feature].unique()

        # Calculate probabilities with Laplace smoothing
        for value in labels:
            count = len(class_data[class_data[feature] == value])
            probabilities[class_label][feature][value] = (count + 1) / (len(class_data) + len(labels))  # Laplace smoothing

def predict_naive_bayes(row, probabilities, priors):
    class_probs = {}
    for label in priors:
        class_prob = priors[label]
        for col, value in row.items():
            # Get conditional probability or a small default if missing
            class_prob *= probabilities[label].get(col, {}).get(value, 1e-6)
        class_probs[label] = class_prob
    return max(class_probs, key=class_probs.get)

# Generate predictions for the test set
predicted_labels = test_X.apply(lambda row: predict_naive_bayes(row, probabilities, priors), axis=1)

print("Test labels (test_y):", test_y.head(), "\nUnique values:", test_y.unique())
print("Predicted labels:", predicted_labels[:5], "\nUnique values:", pd.Series(predicted_labels).unique())

from sklearn.metrics import confusion_matrix
import pandas as pd

# Get unique labels directly from test_y
unique_labels = test_y.unique()

# Generate the confusion matrix
conf_matrix = confusion_matrix(test_y, predicted_labels, labels=unique_labels)

# Convert to DataFrame for easier readability
conf_matrix_df = pd.DataFrame(conf_matrix, index=unique_labels, columns=unique_labels)
print("Confusion Matrix:\n", conf_matrix_df)

"""We both worked on Part 3 equally."""